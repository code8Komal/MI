{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbbbcbaf-0cc2-4c71-b946-a4bbc3aaea90",
   "metadata": {},
   "source": [
    "## Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbae87c9-6448-4c4d-b16a-6c076a944201",
   "metadata": {},
   "source": [
    "## Simple Linear Regression:\n",
    "\n",
    "### Simple linear regression involves predicting the values of one variable (dependent variable) based on the values of another variable (independent variable). The relationship between the two variables is assumed to be linear, and it is represented by a straight line. The equation of a simple linear regression model is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f655f5e-180d-4083-a991-325d7124a00b",
   "metadata": {},
   "source": [
    "Y=β0+β1X+ϵ\n",
    "where:\n",
    "Y is the dependent variable,\n",
    "\n",
    "X is the independent variable,\n",
    "\n",
    "β0 is the intercept,\n",
    "\n",
    "β1 is the slope,\n",
    "\n",
    "ϵ is the error term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41fc865-4594-47a4-aa6e-6be344bf9ed0",
   "metadata": {},
   "source": [
    "## Example:\n",
    "### Consider a scenario where we want to predict a person's weight (Y) based on the number of hours (X) they spend exercising per week. Here, Y is the dependent variable (weight), and X is the independent variable (hours of exercise)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e7451-256b-40fd-afa2-7a7e116e7b4b",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression:\n",
    "\n",
    "Multiple linear regression extends the concept of simple linear regression to more than one independent variable. It is used when there are multiple predictors influencing the dependent variable. The equation for multiple linear regression is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e60bdc-78af-432b-86fa-6759dccffca5",
   "metadata": {},
   "source": [
    "## Y=β0+β1X1+β2X2+…+β nXn+ϵ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e7cd6a-446b-47df-8b2b-212a05446170",
   "metadata": {},
   "source": [
    "## where:\n",
    "Y is the dependent variable,\n",
    "\n",
    "X1,X2,…,Xn are the independent variables,\n",
    "\n",
    "β0 is the intercept,\n",
    "\n",
    "β1,β2,…,βn are the slopes corresponding to each independent variable,\n",
    "\n",
    "ϵ is the error term\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a2de65-4b49-430f-9b39-bffc474d01d8",
   "metadata": {},
   "source": [
    "## Example:\n",
    "### Suppose we want to predict a person's salary (Y) based on their years of experience (X1), education level (X2 ), and age (X3 ). Here, Y is the dependent variable (salary), and X1,X2,X3are the independent variables (years of experience, education level, and age)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077bf89d-723a-4835-ba23-756d0eae29f8",
   "metadata": {},
   "source": [
    "## Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcccf767-3bfe-4cfe-a080-e100dcadbb39",
   "metadata": {},
   "source": [
    "Linear regression relies on several assumptions, and it's important to assess whether these assumptions hold for the model to be valid. Here are the key assumptions of linear regression:\n",
    "\n",
    "1. ## **Linearity:** \n",
    "The relationship between the independent and dependent variables should be linear. This means that changes in the independent variable(s) should result in a constant change in the dependent variable. You can check this assumption by plotting the data and assessing whether a straight line adequately fits the pattern.\n",
    "\n",
    "2. ## **Independence of residuals:** \n",
    "The residuals (the differences between the observed and predicted values) should be independent of each other. In other words, the value of the residuals for one data point should not predict the value of the residuals for another data point. This assumption is often checked using a residual plot or a Durbin-Watson test.\n",
    "\n",
    "3. ## **Homoscedasticity (constant variance of residuals):** \n",
    "The variance of the residuals should remain constant across all levels of the independent variable(s). A plot of residuals against predicted values can help identify whether the spread of residuals is roughly constant. If the spread changes with the predicted values, there might be an issue with homoscedasticity.\n",
    "\n",
    "4. ## **Normality of residuals:** \n",
    "The residuals should be approximately normally distributed. This assumption is not crucial for large sample sizes due to the Central Limit Theorem, but for smaller samples, it's advisable to check the normality of residuals using statistical tests or a Q-Q plot.\n",
    "\n",
    "5. ## **No or little multicollinearity:** \n",
    "In multiple linear regression, the independent variables should not be highly correlated with each other. High multicollinearity can lead to unstable coefficient estimates. Variance Inflation Factor (VIF) is commonly used to check for multicollinearity.\n",
    "\n",
    "## To check these assumptions, you can perform the following steps:\n",
    "\n",
    "- **Residual Analysis:** \n",
    "Examine residual plots (residuals vs. predicted values, residuals vs. each independent variable) to identify patterns or trends.\n",
    "\n",
    "- **Normality Tests:** \n",
    "Use statistical tests (e.g., Shapiro-Wilk) or visual inspections like Q-Q plots to assess the normality of residuals.\n",
    "\n",
    "- **Homoscedasticity Check:**\n",
    "Examine a plot of residuals against predicted values to identify heteroscedasticity.\n",
    "\n",
    "- **Multicollinearity Assessment:** Calculate VIF for each independent variable to identify high multicollinearity.\n",
    "\n",
    "It's crucial to note that linear regression can still provide useful insights even if some assumptions are not perfectly met. However, violating assumptions may affect the accuracy and reliability of the results, so it's essential to interpret the findings with caution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b7f907-db03-4774-9432-17177b4525c4",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f81816-9d35-4ce9-ac48-1c74b5ce86fb",
   "metadata": {},
   "source": [
    "In a linear regression model, the slope and intercept have specific interpretations in the context of the relationship between the independent and dependent variables.\n",
    "\n",
    "1. **Intercept (\\(\\beta_0\\)):**\n",
    "   - The intercept represents the predicted value of the dependent variable when all independent variables are zero.\n",
    "   - In some cases, this interpretation may not be meaningful, especially if zero on the scale of the dependent variable is not a possible or realistic value. However, it is crucial for the mathematical formulation of the regression line.\n",
    "\n",
    "2. **Slope (\\(\\beta_1\\)):**\n",
    "   - The slope represents the change in the dependent variable for a one-unit change in the independent variable, holding other variables constant.\n",
    "   - It quantifies the strength and direction of the linear relationship between the independent and dependent variables.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's consider a real-world scenario where we want to predict the sales (\\(Y\\)) of a product based on the advertising spending (\\(X\\)) in dollars. The linear regression model is given by:\n",
    "\n",
    "\\[ Y = \\beta_0 + \\beta_1X + \\epsilon \\]\n",
    "\n",
    "- \\( \\beta_0 \\): Intercept\n",
    "- \\( \\beta_1 \\): Slope\n",
    "\n",
    "Interpretation:\n",
    "- \\(\\beta_0\\): The intercept represents the predicted sales when the advertising spending is zero. In this context, it could be the baseline sales that might occur without any advertising. However, it's important to note that zero spending might not always make sense in practical scenarios.\n",
    "\n",
    "- \\(\\beta_1\\): The slope represents the change in sales for a one-dollar increase in advertising spending, assuming all other factors remain constant. If \\(\\beta_1\\) is, for example, 0.5, it means that, on average, for every additional dollar spent on advertising, sales are expected to increase by 0.5 units.\n",
    "\n",
    "So, if the model's output is \\(Y = 10 + 0.5X\\), it implies that without any advertising (\\(X=0\\)), the predicted sales would be 10 units, and for each additional dollar spent on advertising, sales are expected to increase by 0.5 units.\n",
    "\n",
    "Interpreting the slope and intercept in the context of the specific variables and units used in the regression equation is crucial for deriving meaningful insights from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be47decb-73a1-42df-9028-40c48d4b7c76",
   "metadata": {},
   "source": [
    "## Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645bc95d-43c4-4e3e-b5b0-b922e7523583",
   "metadata": {},
   "source": [
    "Gradient descent is an optimization algorithm used to minimize the cost or loss function in machine learning models. The primary goal is to find the optimal parameters of a model that minimize the difference between predicted and actual values. It is a key component in training machine learning models, particularly in the context of parameter optimization.\n",
    "\n",
    "Here's a step-by-step explanation of the concept of gradient descent:\n",
    "\n",
    "1. **Cost Function:**\n",
    "   - In machine learning, models are trained to minimize a cost function (also known as a loss or objective function). This function measures the difference between the predicted values of the model and the actual values in the training data.\n",
    "\n",
    "2. **Parameters:**\n",
    "   - Machine learning models have parameters that need to be adjusted during the training process to minimize the cost function. These parameters could be weights in a neural network, coefficients in a linear regression model, etc.\n",
    "\n",
    "3. **Gradient:**\n",
    "   - The gradient represents the partial derivatives of the cost function with respect to each parameter. It indicates the direction of the steepest increase in the cost function. The negative gradient points in the direction of the steepest decrease.\n",
    "\n",
    "4. **Update Rule:**\n",
    "   - Gradient descent iteratively updates the model parameters in the opposite direction of the gradient to minimize the cost function. The update rule for each parameter (\\(\\theta_i\\)) is given by:\n",
    "     \\[ \\theta_i = \\theta_i - \\alpha \\frac{\\partial J}{\\partial \\theta_i} \\]\n",
    "     where:\n",
    "     - \\(\\alpha\\) is the learning rate, a hyperparameter that determines the size of each step.\n",
    "     - \\(\\frac{\\partial J}{\\partial \\theta_i}\\) is the partial derivative of the cost function with respect to the parameter \\(\\theta_i\\).\n",
    "\n",
    "5. **Iterative Process:**\n",
    "   - Steps 3 and 4 are repeated iteratively until the algorithm converges to a minimum of the cost function. Convergence is achieved when the changes in the parameters become very small or when a predetermined number of iterations is reached.\n",
    "\n",
    "Gradient descent can have different variants, such as batch gradient descent, stochastic gradient descent, and mini-batch gradient descent, depending on how it processes the training data. Each iteration involves computing the gradient with respect to the entire dataset (batch), a single data point (stochastic), or a small random subset of the data (mini-batch).\n",
    "\n",
    "The learning rate (\\(\\alpha\\)) is a crucial hyperparameter. A too-small learning rate may result in slow convergence, while a too-large learning rate may cause overshooting and convergence issues.\n",
    "\n",
    "In summary, gradient descent is a fundamental optimization algorithm used in machine learning to iteratively update model parameters, reducing the cost function and improving the model's predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff16e37f-97de-45ea-8c94-fc5d153c3455",
   "metadata": {},
   "source": [
    "## Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849bb58e-519d-4c96-8a64-93b89b3f9b4e",
   "metadata": {},
   "source": [
    "**Multiple Linear Regression Model:**\n",
    "\n",
    "Multiple Linear Regression is an extension of simple linear regression that allows for the modeling of the relationship between a dependent variable (\\(Y\\)) and multiple independent variables (\\(X_1, X_2, \\ldots, X_n\\)). The general form of the multiple linear regression equation is:\n",
    "\n",
    "\\[ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_nX_n + \\epsilon \\]\n",
    "\n",
    "where:\n",
    "- \\( Y \\) is the dependent variable.\n",
    "- \\( X_1, X_2, \\ldots, X_n \\) are the independent variables.\n",
    "- \\( \\beta_0 \\) is the intercept.\n",
    "- \\( \\beta_1, \\beta_2, \\ldots, \\beta_n \\) are the coefficients or slopes associated with each independent variable.\n",
    "- \\( \\epsilon \\) is the error term, representing unobserved factors that affect \\( Y \\) but are not included in the model.\n",
    "\n",
    "**Differences from Simple Linear Regression:**\n",
    "\n",
    "1. **Number of Independent Variables:**\n",
    "   - In simple linear regression, there is only one independent variable (\\(X\\)). In contrast, multiple linear regression involves two or more independent variables (\\(X_1, X_2, \\ldots, X_n\\)).\n",
    "\n",
    "2. **Equation Form:**\n",
    "   - Simple Linear Regression: \\( Y = \\beta_0 + \\beta_1X + \\epsilon \\)\n",
    "   - Multiple Linear Regression: \\( Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_nX_n + \\epsilon \\)\n",
    "   - The equation for multiple linear regression includes multiple independent variables and corresponding coefficients.\n",
    "\n",
    "3. **Interpretation of Coefficients:**\n",
    "   - In simple linear regression, the coefficient (\\(\\beta_1\\)) represents the change in the dependent variable for a one-unit change in the independent variable.\n",
    "   - In multiple linear regression, each coefficient (\\(\\beta_1, \\beta_2, \\ldots, \\beta_n\\)) represents the change in the dependent variable for a one-unit change in the corresponding independent variable, while holding other variables constant.\n",
    "\n",
    "4. **Complexity:**\n",
    "   - Multiple linear regression allows for the modeling of more complex relationships between the dependent variable and multiple predictors. It can capture interactions and dependencies among different independent variables.\n",
    "\n",
    "5. **Matrix Form:**\n",
    "   - Multiple linear regression can be expressed in matrix form as \\( Y = X\\beta + \\epsilon \\), where \\(X\\) is the matrix of independent variables, \\(\\beta\\) is the vector of coefficients, and \\(\\epsilon\\) is the vector of errors.\n",
    "\n",
    "In summary, multiple linear regression extends the simplicity of simple linear regression by accommodating more than one independent variable, providing a more versatile framework for modeling real-world relationships involving multiple predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65880bc-0d2d-4eed-bac1-9832c8cc0112",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9714ead-0e6d-4293-bed6-4f24dd85241a",
   "metadata": {},
   "source": [
    "**Multicollinearity in Multiple Linear Regression:**\n",
    "\n",
    "Multicollinearity is a phenomenon in multiple linear regression where two or more independent variables in the model are highly correlated with each other. This correlation can cause issues in the estimation of the regression coefficients. Specifically, it makes it difficult to determine the individual effect of each independent variable on the dependent variable because their effects are confounded.\n",
    "\n",
    "**Consequences of Multicollinearity:**\n",
    "1. **Unstable Coefficient Estimates:** Small changes in the data can lead to significant changes in the estimated coefficients.\n",
    "2. **Reduced Precision:** The standard errors of the coefficients tend to be large, making it difficult to identify which variables are truly important.\n",
    "3. **Increased p-Values:** The p-values for the affected variables may be inflated, making it harder to reject the null hypothesis.\n",
    "\n",
    "**Detection of Multicollinearity:**\n",
    "Several methods can be employed to detect multicollinearity:\n",
    "\n",
    "1. **Correlation Matrix:** Examine the correlation matrix among the independent variables. High correlation coefficients (close to +1 or -1) indicate potential multicollinearity.\n",
    "  \n",
    "2. **Variance Inflation Factor (VIF):** Calculate the VIF for each independent variable. VIF measures how much the variance of an estimated regression coefficient increases if your predictors are correlated. VIF values greater than 10 or 5 are often considered indicative of multicollinearity.\n",
    "\n",
    "3. **Tolerance:** Tolerance is another metric used to assess multicollinearity. It is the reciprocal of the VIF (\\(Tolerance = 1/VIF\\)). Tolerance values close to 1 indicate low multicollinearity.\n",
    "\n",
    "**Addressing Multicollinearity:**\n",
    "\n",
    "1. **Remove Redundant Variables:** If two or more variables are highly correlated, consider removing one of them from the model.\n",
    "\n",
    "2. **Combine Variables:** Instead of using multiple correlated variables, you can create a composite variable that represents their combined effect.\n",
    "\n",
    "3. **Feature Selection:** Use techniques like backward elimination or forward selection to choose a subset of features based on their importance and multicollinearity.\n",
    "\n",
    "4. **Regularization Techniques:** Techniques like Ridge Regression or Lasso Regression include regularization terms that penalize large coefficients, helping to mitigate multicollinearity.\n",
    "\n",
    "5. **Collect More Data:** Increasing the size of the dataset can sometimes help reduce the impact of multicollinearity.\n",
    "\n",
    "It's important to note that the severity of multicollinearity and the choice of the appropriate method for addressing it depend on the specific context and goals of the analysis. Addressing multicollinearity can lead to more reliable and interpretable regression results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591d8e8e-032b-4072-8b06-d226f83ee38c",
   "metadata": {},
   "source": [
    "## Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cddb84d-3cb4-4aab-bca0-7f8eade09bb6",
   "metadata": {},
   "source": [
    "**Polynomial Regression Model:**\n",
    "\n",
    "Polynomial regression is a type of regression analysis in which the relationship between the independent variable (\\(X\\)) and the dependent variable (\\(Y\\)) is modeled as an \\(n\\)-th degree polynomial. The general form of a polynomial regression equation is:\n",
    "\n",
    "\\[ Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\ldots + \\beta_nX^n + \\epsilon \\]\n",
    "\n",
    "where:\n",
    "- \\( Y \\) is the dependent variable.\n",
    "- \\( X \\) is the independent variable.\n",
    "- \\( \\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_n \\) are the coefficients.\n",
    "- \\( \\epsilon \\) is the error term.\n",
    "\n",
    "In this model, the relationship between \\(X\\) and \\(Y\\) is not assumed to be linear; instead, it is modeled as a polynomial function of degree \\(n\\).\n",
    "\n",
    "**Differences from Linear Regression:**\n",
    "\n",
    "1. **Functional Form:**\n",
    "   - Linear Regression: \\( Y = \\beta_0 + \\beta_1X + \\epsilon \\)\n",
    "   - Polynomial Regression: \\( Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\ldots + \\beta_nX^n + \\epsilon \\)\n",
    "\n",
    "2. **Linearity vs. Non-Linearity:**\n",
    "   - Linear regression assumes a linear relationship between the independent and dependent variables. Polynomial regression allows for non-linear relationships, capturing more complex patterns in the data.\n",
    "\n",
    "3. **Flexibility:**\n",
    "   - Linear regression models linear relationships, which may not adequately represent certain data patterns. Polynomial regression provides greater flexibility by allowing the model to fit curves and bends in the data.\n",
    "\n",
    "4. **Degree of the Polynomial:**\n",
    "   - In polynomial regression, the degree of the polynomial (\\(n\\)) is a hyperparameter that needs to be specified. The choice of \\(n\\) determines the complexity of the model and its ability to fit the data.\n",
    "\n",
    "5. **Interpretability:**\n",
    "   - Linear regression coefficients have straightforward interpretations: \\(\\beta_1\\) represents the change in \\(Y\\) for a one-unit change in \\(X\\). In polynomial regression, the interpretation becomes more complex as higher-degree terms are introduced.\n",
    "\n",
    "**Use Cases:**\n",
    "- Polynomial regression is often employed when the true relationship between variables is expected to be more complex than a simple straight line.\n",
    "- It is suitable for capturing patterns such as curves, bends, or oscillations in the data.\n",
    "\n",
    "**Considerations:**\n",
    "- While polynomial regression can capture complex relationships, higher-degree polynomials may lead to overfitting, especially with limited data.\n",
    "- The choice of the degree of the polynomial should be guided by model performance metrics and a balance between fitting the training data and generalizing to new, unseen data. Regularization techniques can be employed to control overfitting.\n",
    "\n",
    "In summary, polynomial regression is a flexible extension of linear regression that allows for modeling non-linear relationships between variables by introducing polynomial terms of various degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfdec36-b4a1-40bd-9809-53165c1ff3b6",
   "metadata": {},
   "source": [
    "## Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cbe952-7874-4218-85d3-df0fb80a3e96",
   "metadata": {},
   "source": [
    "**Advantages of Polynomial Regression:**\n",
    "\n",
    "1. **Flexibility:** Polynomial regression can capture non-linear relationships between variables, providing more flexibility to model complex patterns in the data.\n",
    "\n",
    "2. **Improved Fit to Data:** In cases where the relationship between the independent and dependent variables is not linear, polynomial regression can result in a better fit to the data compared to linear regression.\n",
    "\n",
    "3. **Increased Expressiveness:** Polynomial regression allows the model to express a wider range of relationships, including curves, bends, and oscillations, which linear regression may not capture.\n",
    "\n",
    "**Disadvantages of Polynomial Regression:**\n",
    "\n",
    "1. **Overfitting:** Higher-degree polynomials can lead to overfitting, where the model fits the training data too closely, capturing noise rather than the underlying pattern. This can result in poor generalization to new, unseen data.\n",
    "\n",
    "2. **Complexity and Interpretability:** As the degree of the polynomial increases, the model becomes more complex, making it harder to interpret. Coefficients of higher-degree terms may not have meaningful or easily interpretable explanations.\n",
    "\n",
    "3. **Sensitivity to Outliers:** Polynomial regression can be sensitive to outliers, leading to exaggerated effects of extreme values on the model.\n",
    "\n",
    "4. **Computational Complexity:** As the degree of the polynomial increases, the computational complexity of fitting the model also increases. This may become a concern, especially with large datasets.\n",
    "\n",
    "**When to Use Polynomial Regression:**\n",
    "\n",
    "1. **Non-Linear Relationships:** When the true relationship between the independent and dependent variables is non-linear, polynomial regression can be a suitable choice.\n",
    "\n",
    "2. **Curved or Bending Patterns:** If the data exhibits curved or bending patterns that linear regression cannot capture, polynomial regression may be more appropriate.\n",
    "\n",
    "3. **Limited Data Points:** In situations where there are limited data points, polynomial regression may provide a better fit, even if it carries a risk of overfitting.\n",
    "\n",
    "4. **Exploratory Data Analysis:** Polynomial regression can be useful in exploratory data analysis to understand the underlying patterns in the data and guide further modeling decisions.\n",
    "\n",
    "5. **Regularization Techniques:** When regularization techniques (e.g., Ridge or Lasso regression) are employed to control overfitting, polynomial regression can strike a balance between fitting the training data and preventing overfitting.\n",
    "\n",
    "In summary, the choice between linear and polynomial regression depends on the underlying data patterns, the nature of the relationship between variables, and considerations regarding model complexity and interpretability. While polynomial regression offers more flexibility, it should be used judiciously to avoid overfitting and ensure meaningful model interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562008a3-8f7b-44e2-b318-4bf30aa53602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bc874e-2117-4a55-bfae-0f9f68514439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
