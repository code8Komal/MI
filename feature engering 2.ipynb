{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0846f48b-d92f-410c-82cc-a31c4ce6d296",
   "metadata": {},
   "source": [
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41351f4c-2905-4a33-b405-4735baa1364c",
   "metadata": {},
   "source": [
    "## Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform the values of a numerical feature into a specific range, typically between 0 and 1. It is done by rescaling the original feature values in such a way that the minimum value becomes 0, the maximum value becomes 1, and all other values are scaled proportionally between these two extremes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064057a3-c27b-4a13-a6b9-9be77305135d",
   "metadata": {},
   "source": [
    "## The formula for Min-Max scaling is as follows for each data point x:\n",
    "\n",
    "## x_normalized=x−min(x)/(max(x)−min(x))\n",
    "\n",
    "Where:\n",
    "## x_normalized is the normalized value of x.\n",
    "## x is the original value.\n",
    "## min(x) is the minimum value in the dataset for that particular feature.\n",
    "## max(x) is the maximum value in the dataset for that particular feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192a06b3-5f4d-401d-bc67-25ddb046a2ac",
   "metadata": {},
   "source": [
    "### The purpose of Min-Max scaling is to make the features more comparable and to ensure that they have the same scale, which can be beneficial for machine learning algorithms that rely on distances or gradients, like k-nearest neighbors or neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033229f6-f12e-4bfc-9fd2-457b63134616",
   "metadata": {},
   "source": [
    "## Example:\n",
    "### Let's say you have a dataset containing the ages of people with values ranging from 18 to 60, and you want to normalize the age feature using Min-Max scaling. The minimum age (min(x)) is 18, and the maximum age (max(x)) is 60. To normalize an age of 30 using Min-Max scaling, you would apply the formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7655a218-ecbf-4f3d-8cfe-71f6ebc67030",
   "metadata": {},
   "source": [
    "### x_normalized=(30-18)/(60-18)=12/42=0.2857\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532ab885-3960-4cc6-9d78-20c98baf9203",
   "metadata": {},
   "source": [
    "### So, the age of 30 would be normalized to approximately 0.2857 in the range [0, 1]. You would perform this transformation for all age values in your dataset to bring them into the desired range for further analysis or modeling.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00908a60-3024-4bc9-aa53-6c59236265dd",
   "metadata": {},
   "source": [
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c8e9aa-bb2d-44df-8000-cee562852cca",
   "metadata": {},
   "source": [
    "### Unit vector scaling, also known as vector normalization, is a feature scaling technique that transforms the data in such a way that each data point (vector) has a length or magnitude of 1. This technique is commonly used in various machine learning algorithms, especially those sensitive to the magnitude or Euclidean distance between data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd24991-d928-407f-afbd-29d0f6871e88",
   "metadata": {},
   "source": [
    "## The formula for unit vector scaling (unit vector transformation) is as follows for each data point x:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898248d2-86f5-4cc7-a0ea-d91dbbfbcfad",
   "metadata": {},
   "source": [
    "## x_unit vector=x/∥x∥\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029545c6-a228-4a19-a37f-6ab9819da208",
   "metadata": {},
   "source": [
    "## Where:\n",
    "## x_unit vector is the unit vector representation of x.\n",
    "## x is the original data point.\n",
    "## ∥x∥ represents the Euclidean norm or magnitude of the vector x.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ca32c-651a-43df-b05b-1fe5b56ea6fa",
   "metadata": {},
   "source": [
    "## The primary difference between Min-Max scaling and unit vector scaling lies in the range of the transformed values and the purpose:\n",
    "\n",
    "### Min-Max scaling: It scales data to a predefined range (e.g., [0, 1] or [-1, 1]), which is useful for comparing and interpreting data within a specific range. The transformed values are bounded within this range, and the relative differences between the values are preserved.\n",
    "\n",
    "### Unit vector scaling: It scales data in such a way that each data point has a magnitude of 1. This technique is used to ensure that the direction or relative orientation of data points is preserved, rather than their absolute magnitudes. It's often used in algorithms where the magnitude or distance between data points is irrelevant, and only the direction matters, such as in clustering or some dimensionality reduction techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5e331a-95db-4263-b5cf-f5bb0e1eb780",
   "metadata": {},
   "source": [
    "## Example:\n",
    "### Let's say you have a dataset with two numerical features, x1 and x2  , and you want to perform unit vector scaling on a data point(3,4)(3,4). First, you calculate the Euclidean norm (∥x∥) of the data point:\n",
    "\n",
    "∥x∥= sart(3**2+4**2)=5 \n",
    "\n",
    "## Then, you divide each component of the data point by the norm to obtain the unit vector:\n",
    "\n",
    "## x_unit vector=(3/5,4/5)\n",
    "\n",
    "### So, the original data point (3,4) is transformed into the unit vector (3/5,4/5). The magnitude of the unit vector is 1, and its direction is preserved. This can be particularly useful in cases where you want to focus on the relative relationships between data points without being influenced by their magnitudes.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3fbdcd-e7d2-45d2-b772-e6e2d128de95",
   "metadata": {},
   "source": [
    "## Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc17cf6-191a-4a2a-b6a7-1b28b4c5a7c7",
   "metadata": {},
   "source": [
    "### PCA (Principal Component Analysis) is a dimensionality reduction technique used in the field of data analysis and machine learning. Its primary purpose is to reduce the number of features (dimensions) in a dataset while retaining as much of the original data's variance as possible. PCA accomplishes this by transforming the original features into a new set of orthogonal linear combinations called principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4e478c-6cef-427c-8dbe-cdac456dbb60",
   "metadata": {},
   "source": [
    "## Here's how PCA works:\n",
    "\n",
    "## Data Centering: \n",
    "First, PCA typically involves centering the data by subtracting the mean from each feature. Centering is important to ensure that the first principal component explains the direction of maximum variance in the data.\n",
    "\n",
    "## Covariance Matrix Calculation:\n",
    "PCA calculates the covariance matrix of the centered data. The covariance matrix provides information about how features are related to each other.\n",
    "\n",
    "## Eigenvalue and Eigenvector Decomposition: \n",
    "PCA then performs an eigenvalue and eigenvector decomposition of the covariance matrix. The eigenvectors represent the principal components, and the corresponding eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "## Selecting Principal Components: \n",
    "PCA sorts the eigenvectors by their associated eigenvalues in descending order. The principal components are selected based on how much variance they explain. Typically, you choose a subset of the top principal components that explain most of the variance.\n",
    "\n",
    "## Projection:\n",
    "The data is then projected onto the selected principal components, effectively reducing the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e34e39-d3ac-4a0b-8b97-222d057d5e4b",
   "metadata": {},
   "source": [
    "## PCA is often used for the following purposes:\n",
    "\n",
    "## Dimensionality Reduction: \n",
    "By selecting a subset of the top principal components, PCA reduces the dimensionality of the data while retaining most of the variance. This can help simplify the dataset and reduce noise.\n",
    "\n",
    "## Feature Engineering: \n",
    "Principal components can sometimes reveal patterns or relationships between original features that are not immediately apparent. They can serve as new features for modeling.\n",
    "\n",
    "## Data Visualization: \n",
    "PCA can be used to visualize high-dimensional data in a lower-dimensional space, making it easier to explore and understand the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6d2077-b71f-45d9-bf05-42914d34670d",
   "metadata": {},
   "source": [
    "## Example:\n",
    "### Suppose you have a dataset with three features: height, weight, and age, and you want to use PCA for dimensionality reduction. After centering the data and calculating the covariance matrix, you find the following eigenvalues and eigenvectors:\n",
    "\n",
    "### Eigenvalues:\n",
    "\n",
    "First principal component: 10\n",
    "Second principal component: 5\n",
    "Third principal component: 2\n",
    "\n",
    "## Eigenvectors:\n",
    "\n",
    "First principal component: [0.6, 0.6, 0.5]\n",
    "Second principal component: [0.7, -0.7, 0]\n",
    "Third principal component: [0.3, 0, -0.9]\n",
    "\n",
    "in this case, you might choose to keep the first two principal components, as they explain the majority of the variance in the data. You can then project your original data onto these two components, effectively reducing the dimensionality of your dataset from three features to two features while preserving most of the data's variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2326f4-e413-4841-9803-8733d25dda31",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd96e506-0cf8-4c52-899c-1ec737f9f700",
   "metadata": {},
   "source": [
    "### PCA (Principal Component Analysis) is often used as a feature extraction technique in machine learning and data analysis. Feature extraction refers to the process of transforming the original features in a dataset into a smaller set of new features while retaining the most important information. PCA can be used for feature extraction by generating a reduced set of features called principal components, which capture the most significant variations in the data. These principal components can then be used as a reduced feature set for further analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8397c72a-4cfe-4700-97d5-38c1c9efafa0",
   "metadata": {},
   "source": [
    "The relationship between PCA and feature extraction can be summarized as follows:\n",
    "\n",
    "## Dimensionality Reduction: \n",
    "PCA reduces the dimensionality of the dataset by transforming the original features into a set of linearly uncorrelated principal components.\n",
    "\n",
    "## Retaining Information: \n",
    "PCA ranks the principal components by the amount of variance they explain. The first few principal components typically capture the most significant information in the data, while the remaining components capture less important variations.\n",
    "\n",
    "## Feature Selection:\n",
    "Instead of using all the original features, you can select a subset of the top principal components to use as the reduced feature set. This selection is based on the explained variance or the number of components desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de18c9c2-21b0-4acc-ae88-410dea72898b",
   "metadata": {},
   "source": [
    "Example:\n",
    "Let's consider an example with a dataset of images. Each image contains pixel values for a face, and you want to reduce the dimensionality of the dataset while preserving the essential facial features. You can use PCA for feature extraction:\n",
    "\n",
    "### Data Preprocessing:\n",
    "#### Flatten each image into a vector, so each original feature represents a pixel.\n",
    "\n",
    "### Apply PCA: \n",
    "#### Calculate the principal components of the pixel vectors. These principal components will represent patterns in the images, such as the orientation of facial features, lighting variations, or expressions.\n",
    "\n",
    "### Variance Explained:\n",
    "#### PCA provides the explained variance for each principal component. You can plot a scree plot or use a threshold to determine how many principal components to keep. For example, you might decide to keep the top 20 principal components, which explain 95% of the variance.\n",
    "\n",
    "### Reduced Feature Set: \n",
    "#### Use the selected principal components (e.g., 20 components) as your new reduced feature set for representing the images.\n",
    "\n",
    "By using PCA for feature extraction, you've transformed the original pixel values into a smaller set of features that capture the most important information in the images. This can reduce noise and computational complexity in machine learning tasks while still preserving the key characteristics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09473b9f-8079-4ef4-960f-ef06d752ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5608611c-e525-431c-9f5e-ed53a4bc4bab",
   "metadata": {},
   "source": [
    "### Min-Max scaling is a data preprocessing technique used to normalize numerical features within a specific range, typically between 0 and 1. This is particularly useful when working with features that have different units or scales, such as price, rating, and delivery time in a food delivery recommendation system dataset. The goal of Min-Max scaling is to transform these features so that they are all on the same scale, making it easier for machine learning algorithms to process and interpret the data.\n",
    "\n",
    "### Here's how you can use Min-Max scaling to preprocess the data for your food delivery recommendation system project:\n",
    "\n",
    "## Identify the features: \n",
    "#### First, identify the numerical features in your dataset that need to be scaled. In , this would be price, rating, and delivery time.\n",
    "\n",
    "## Understand the range:\n",
    "#### Determine the minimum and maximum values for each of the features. For example, find the minimum and maximum prices, ratings, and delivery times in your dataset.\n",
    "\n",
    "## Calculate the Min-Max scaling transformation: \n",
    "#### For each feature, apply the Min-Max scaling transformation using the following formula:\n",
    "\n",
    "## Scaled_value = (X - X_min) / (X_max - X_min)\n",
    "X: The original value of the feature.\n",
    "X_min: The minimum value of the feature in the dataset.\n",
    "X_max: The maximum value of the feature in the dataset.\n",
    "Apply the transformation: Apply the transformation to each data point in your dataset for the selected features. This will result in scaled values for each feature, where the minimum value becomes 0, and the maximum value becomes 1.\n",
    "\n",
    "## Apply Min-Max Scaling:\n",
    "#### Using the formula from step 3, apply Min-Max scaling to each data point for the features you want to preprocess (price, rating, and delivery time). This will ensure that all these features are rescaled within the specified range ([0, 1] in most cases).\n",
    "\n",
    "## Implement Min-Max Scaling:\n",
    "#### Depending on your programming environment or tools, you can implement Min-Max scaling using libraries like Scikit-Learn in Python or manually in code. Here's an example of how you might do it in Python using Scikit-Learn:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c89775c-80f9-41c5-9477-68b6143644ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to your data and transform the features\n",
    "scaled_data = scaler.fit_transform(your_data[['price', 'rating', 'delivery_time']])\n",
    "\n",
    "# The scaled_data now contains your scaled features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9ccfa7-35c1-4661-97d7-eaf7ee654894",
   "metadata": {},
   "source": [
    "## Store the scaled data: \n",
    "#### Replace the original values of the features with their corresponding scaled values in your dataset. These scaled features are now ready to be used in your recommendation system model.\n",
    "\n",
    "##### Min-Max scaling has the advantage of preserving the relationships between data points while ensuring that all features are on the same scale. This can lead to improved model performance, as the model can more effectively consider and compare these features when making recommendations, without any feature dominating due to its original scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63192e8e-ea5b-4893-8f18-537c0f7c93db",
   "metadata": {},
   "source": [
    "## Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0694c5-ee29-4394-9aae-3f6886863dcf",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in machine learning and data analysis to reduce the number of features in a dataset while retaining as much of the original data's information as possible. When working on a project to predict stock prices with a dataset that contains numerous features like company financial data and market trends, you can use PCA as follows to reduce dimensionality:\n",
    "\n",
    "## Data Preprocessing:\n",
    "Before applying PCA, it's essential to preprocess your dataset. This typically involves handling missing values, standardizing or normalizing the data, and removing any outliers. Make sure your data is in a suitable format for PCA, as it assumes that the features are numeric and continuous.\n",
    "\n",
    "## Calculate the Covariance Matrix:\n",
    "PCA is based on the covariance matrix of the features. Calculate the covariance matrix for your dataset. The covariance matrix describes the relationships between the features, showing how they vary together.\n",
    "\n",
    "## Eigendecomposition of the Covariance Matrix:\n",
    "Perform an eigendecomposition of the covariance matrix to find the eigenvalues and eigenvectors. The eigenvalues represent the amount of variance explained by each principal component, and the eigenvectors determine the direction of the principal components.\n",
    "\n",
    "## Sort Eigenvalues and Eigenvectors:\n",
    "Sort the eigenvalues in descending order and their corresponding eigenvectors accordingly. This step helps you identify the principal components responsible for the most variance in the data.\n",
    "\n",
    "## Select the Number of Principal Components:\n",
    "Decide on the number of principal components you want to retain. Typically, you aim to capture a significant portion of the variance in the data while reducing dimensionality. This decision can be based on a desired explained variance threshold (e.g., 95% of the variance) or a visual inspection of the scree plot, which shows the variance explained by each component.\n",
    "\n",
    "## Project the Data:\n",
    "Using the selected eigenvectors, project your original dataset onto a new feature space consisting of a reduced number of principal components. This is done by taking the dot product of your data with the selected eigenvectors.\n",
    "\n",
    "## Create a New Dataset:\n",
    "Form a new dataset with the projected data, where each instance is now represented by a reduced set of features (the principal components) rather than the original features.\n",
    "\n",
    "## Train and Evaluate Your Model:\n",
    "With the reduced-dimensional dataset, you can now train and evaluate your stock price prediction model. Using fewer features can often lead to faster training times and reduced overfitting. Ensure that you maintain a validation dataset to assess the model's performance effectively.\n",
    "\n",
    "## Inverse Transform (Optional):\n",
    "If necessary, you can use the inverse transformation to go back from the reduced-dimensional space to the original feature space, which might be useful for interpreting the model's results or for feature engineering.\n",
    "\n",
    "PCA helps reduce the dimensionality of your dataset by focusing on the most critical information while eliminating noise and multicollinearity. This can be particularly beneficial when dealing with a large number of features, as is often the case in financial data and stock price prediction tasks. However, it's important to strike a balance between dimensionality reduction and information retention to ensure that your model remains accurate and useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c2e40b-d41d-47d7-bd3c-f13a31341223",
   "metadata": {},
   "source": [
    "## Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed63167-ad5c-4718-8047-e7bee60b735d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "#To perform Min-Max scaling on the given dataset and transform the values to a range of -1 to 1, you can use the following Python code\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Given dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Calculate the minimum and maximum values in the dataset\n",
    "min_value = np.min(data)\n",
    "max_value = np.max(data)\n",
    "\n",
    "# Define the target range (-1 to 1)\n",
    "new_min = -1\n",
    "new_max = 1\n",
    "\n",
    "# Apply Min-Max scaling transformation\n",
    "scaled_data = ((data - min_value) / (max_value - min_value)) * (new_max - new_min) + new_min\n",
    "\n",
    "print(scaled_data)\n",
    "\n",
    "#This code first calculates the minimum and maximum values in the dataset and then applies the Min-Max scaling transformation to map the values to the range of -1 to 1. The resulting scaled_data array will contain the transformed values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3fdcfd-46be-45bb-b39e-6e61a0f732ce",
   "metadata": {},
   "source": [
    "## Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11303a6-5297-49f1-b7d5-98f88de52167",
   "metadata": {},
   "source": [
    "When performing feature extraction using Principal Component Analysis (PCA) on a dataset with features like height, weight, age, gender, and blood pressure, the number of principal components to retain depends on several factors, including the desired trade-off between dimensionality reduction and information retention. Here are some considerations for deciding how many principal components to keep:\n",
    "\n",
    "## Data Variability:\n",
    "The primary goal of PCA is to capture as much of the variability in the data as possible while reducing the dimensionality. You should start by examining the explained variance ratio of each principal component. This ratio tells you how much of the total variance in the dataset is explained by each component. You typically sort these ratios in decreasing order.\n",
    "\n",
    "## Explained Variance Threshold:\n",
    "Choose a desired level of explained variance that you want to retain. For example, you might aim to retain 95% or 99% of the total variance. This threshold is subjective and depends on the specific requirements of your analysis.\n",
    "\n",
    "## Cumulative Explained Variance:\n",
    "Calculate the cumulative explained variance by summing the explained variance ratios of the principal components from highest to lowest. You can use a scree plot or a cumulative variance plot to visualize the variance explained by each component and decide how many components are needed to meet your chosen threshold.\n",
    "\n",
    "## Retain Sufficient Information:\n",
    "The number of principal components to keep should be sufficient to capture the essential information in your data. This includes retaining features that are most relevant to your analysis and reducing noise or redundancy in the data.\n",
    "\n",
    "## Practical Considerations:\n",
    "Consider the practical aspects of using the reduced-dimensional data. Fewer dimensions can lead to faster training times and simpler models, but it may also result in a loss of interpretability. In some cases, you may need to strike a balance between dimensionality reduction and the interpretability of your results.\n",
    "\n",
    "## Cross-Validation:\n",
    "If your data analysis involves machine learning or statistical modeling, consider using cross-validation to assess the performance of your model with different numbers of retained principal components. This can help you determine which configuration leads to the best model performance.\n",
    "\n",
    "The choice of the number of principal components is somewhat subjective and context-specific. You may need to experiment with different values and evaluate the impact on your specific analysis or modeling task. Retaining enough principal components to explain a high proportion of the variance is a common guideline. In practice, you might find that two or three principal components capture most of the relevant information, especially for numerical features like height, weight, and blood pressure. However, gender, being a categorical feature, may not be well-suited for PCA and might be better handled separately in your analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618898aa-bef5-4ad7-87cb-a4de03c48863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
